{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os import walk\n",
    "from os.path import splitext\n",
    "\n",
    "from load_data import get_metadata_from_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset directory\n",
    "data_dir = '/media/feliximmohr/Storage/master_thesis/generated/database/hdf5/'\n",
    "export_dir = data_dir+'../single/'\n",
    "# load filelist\n",
    "filelist = []\n",
    "for (dirpath, dirnames, filenames) in walk(data_dir):\n",
    "    filelist.extend(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# write each csv file's data to seperate HDF5 file\n",
    "for name in filelist:\n",
    "    # load csv\n",
    "    df = pd.read_csv(data_dir+name)\n",
    "    # save to HDF5\n",
    "    name_new, _ = splitext(name)\n",
    "    df.to_hdf(export_dir+name_new+'.h5', 'data', mode='w', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# append to single large HDF5 file; each DF with individual key\n",
    "for name in filelist:\n",
    "    m, _, p, _ = get_metadata_from_filename(name)\n",
    "    key = m+'_'+p\n",
    "    # load file\n",
    "    df = pd.read_hdf(data_dir+name)\n",
    "    # save to HDF5\n",
    "    df.to_hdf(export_dir+'database.h5', key, mod='a', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# append to single large HDF5 file; all DF appended with single key\n",
    "\n",
    "h5_file = pd.HDFStore(export_dir+'database.h5')#, complevel=5, complib='blosc')\n",
    "\n",
    "counter = 1\n",
    "for name in filelist:\n",
    "    print('Appending Table {}'.format(counter))\n",
    "    df = pd.read_hdf(data_dir+name)\n",
    "    df2 = df.astype({'x':'float64', 'y':'float64'})\n",
    "    h5_file.append('database', df2)#, complevel=5, complib='blosc')\n",
    "    counter += 1\n",
    "    \n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_hdf(export_dir+'database.h5')#,where=[np.arange(720000).tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = ['col{}'.format(i) for i in range(100)]\n",
    "data = np.random.randn(100000).reshape(1000, 100)\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# many tables, generator\n",
    "def get_generator(df, n=1000):\n",
    "    for x in range(n):\n",
    "        yield df\n",
    "\n",
    "table_reader = get_generator(df, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/feliximmohr/Storage/master_thesis/generated/database/raw/'\n",
    "feature_data = pd.read_csv(data_dir+'feature_data.csv')\n",
    "target_data = pd.read_csv(data_dir+'target_data.csv')\n",
    "position_table = pd.read_csv(data_dir+'position_table.csv')\n",
    "condition_table = pd.read_csv(data_dir+'condition_table.csv')\n",
    "ID_reference_table = pd.read_csv(data_dir+'ID_reference_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_table.to_hdf(data_dir+'database_raw.h5', 'position_table', mode='a', format='table')\n",
    "condition_table.to_hdf(data_dir+'database_raw.h5', 'condition_table', mode='a', format='table')\n",
    "ID_reference_table.to_hdf(data_dir+'database_raw.h5', 'ID_reference_table', mode='a', format='table')\n",
    "feature_data.to_hdf(data_dir+'database_raw.h5', 'feature_data', mode='a', format='table')\n",
    "target_data.to_hdf(data_dir+'database_raw.h5', 'target_data', mode='a', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
