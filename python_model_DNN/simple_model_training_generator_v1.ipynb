{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import walk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras import backend as K #for custom loss function\n",
    "\n",
    "import tensorflow as tf #for custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(Sequence):\n",
    "    \"\"\"\n",
    "    Generator class to process large datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filenames, feat_col_labels, target_col_labels, batch_size):\n",
    "        self.filenames = filenames\n",
    "        self.feat_col_labels = feat_col_labels\n",
    "        self.target_col_labels = target_col_labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_csv(self.filenames[idx])# * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        batch_x = data[self.feat_col_labels].values\n",
    "        batch_y = data[self.target_col_labels].values\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_wrap_angle(y_true, y_pred):\n",
    "    \"\"\"Custom loss function based on MSE but with angles wrapped to 360 degree.\"\"\"\n",
    "    diff = y_pred - y_true\n",
    "    if K.greater(diff,180) is not None:\n",
    "        diff = diff - 360\n",
    "    elif K.less(diff,-180) is not None:\n",
    "        diff = diff + 360\n",
    "    return K.mean(K.square(diff), axis=-1)\n",
    "\n",
    "def mae_wrap_angle(y_true, y_pred):\n",
    "    \"\"\"Custom loss function based on MAE but with angles wrapped to 360 degree.\"\"\"\n",
    "    diff = y_pred - y_true\n",
    "    if K.greater(diff,180) is not None:\n",
    "        diff = diff - 360\n",
    "    elif K.less(diff,-180) is not None:\n",
    "        diff = diff + 360\n",
    "    return K.mean(K.abs(diff), axis=-1)\n",
    "\n",
    "def mse_wrap_angle2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function based on MSE but with angles wrapped to 360 degree.\n",
    "    Tensorflows atan2 is used. Therefore conversion to radiant and back is performed.\n",
    "    \"\"\"\n",
    "    a = tf_deg2rad(y_pred)\n",
    "    b = tf_deg2rad(y_true)\n",
    "    diff = tf.atan2(tf.sin(a - b), tf.cos(a - b))\n",
    "    diff = tf_rad2deg(diff)\n",
    "    return K.mean(K.square(diff), axis=-1)\n",
    "\n",
    "def mae_wrap_angle2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function based on MAE but with angles wrapped to 360 degree.\n",
    "    Tensorflows atan2 is used. Therefore conversion to radiant and back is performed.\n",
    "    \"\"\"\n",
    "    a = tf_deg2rad(y_pred)\n",
    "    b = tf_deg2rad(y_true)\n",
    "    diff = tf.atan2(tf.sin(a - b), tf.cos(a - b))\n",
    "    diff = tf_rad2deg(diff)\n",
    "    return K.mean(K.abs(diff), axis=-1)\n",
    "\n",
    "def tf_deg2rad(a):\n",
    "    \"\"\"Tensorflow compatible conversion from degree to radians.\"\"\"\n",
    "    return tf.divide(tf.multiply(a,np.pi),180)\n",
    "\n",
    "def tf_rad2deg(a):\n",
    "    \"\"\"Tensorflow compatible conversion from degree to radians.\"\"\"\n",
    "    return tf.divide(tf.multiply(a,180),np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model():\n",
    "    \"\"\"Define simple model\"\"\"\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(96,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # show info\n",
    "    model.summary()\n",
    "    # compile model\n",
    "    model.compile(loss=mse_wrap_angle2,\n",
    "                  optimizer=SGD(),\n",
    "                  metrics = [mae_wrap_angle2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "batch_size = 1\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset directory\n",
    "data_dir = '/media/feliximmohr/Storage/master_thesis/generated/database/' \n",
    "# load filelist\n",
    "filelist = []\n",
    "for (dirpath, dirnames, filenames) in walk(data_dir):\n",
    "    filelist.extend(filenames)\n",
    "    \n",
    "# load column labels only\n",
    "column_label = pd.read_csv(data_dir+filelist[1], nrows=1).columns.tolist()\n",
    "feature_label = [s for s in column_label if (\"ILD\"  in s or \"ITD\" in s or \"IC\" in s)]\n",
    "target_label = 'Localization_Azimuth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = 20*720000\n",
    "num_valid_samples = 10*720000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator on batches of dataset\n",
    "\n",
    "model = simple_model()\n",
    "\n",
    "train_filenames = [data_dir + s for s in filelist[0:20]]\n",
    "valid_filenames = [data_dir + s for s in filelist[20:30]]\n",
    "\n",
    "# define generators\n",
    "train_batch_generator = data_generator(train_filenames, feature_label, target_label, 1)\n",
    "valid_batch_generator = data_generator(valid_filenames, feature_label, target_label, 1)\n",
    "\n",
    "history = model.fit_generator(generator = train_batch_generator,\n",
    "                              steps_per_epoch = (num_train_samples),# // batch_size),\n",
    "                              epochs = num_epochs,\n",
    "                              verbose = 1,\n",
    "                              validation_data = valid_batch_generator,\n",
    "                              validation_steps = (num_valid_samples),# // batch_size),\n",
    "                              use_multiprocessing = True,\n",
    "                              workers = 2,\n",
    "                              max_queue_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization + train_test_split\n",
    "data = pd.read_csv(data_dir+filelist[60])\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(data[feature_label])\n",
    "std_scale = preprocessing.StandardScaler().fit(data[feature_label])\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3)\n",
    "train_minmax = minmax_scale.transform(train[feature_label])\n",
    "test_minmax = minmax_scale.transform(test[feature_label])\n",
    "train_std = std_scale.transform(train[feature_label])\n",
    "test_std = std_scale.transform(test[feature_label])\n",
    "\n",
    "x_train = train_minmax\n",
    "y_train = train[target_label]\n",
    "x_test = test_minmax\n",
    "y_test = test[target_label]\n",
    "\n",
    "#data_minmax = minmax_scale.transform(data[feature_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test mae w/o wrap:', score[1])\n",
    "print('Test mae w wrap:', score[2])\n",
    "\n",
    "# plot train history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history to json file\n",
    "import json\n",
    "with open('file.json', 'w') as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
