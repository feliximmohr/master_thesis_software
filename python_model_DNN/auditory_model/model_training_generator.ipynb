{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Neural Network based Auditory Model\n",
    "\n",
    "## An Auditory Model for  Azimuthal Localisation in Sound Field Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.custom_loss import mae_wrap_angle, mse_wrap_angle\n",
    "from utils.load_data_raw import DataGenerator_raw, load_raw_ft_h5, load_raw_IDs_h5\n",
    "from utils.dataset_split import create_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKER = 4\n",
    "\n",
    "# configure tf session; disable usage of GPU\n",
    "config = tf.ConfigProto(device_count={'CPU': NUM_WORKER,\n",
    "                                      'GPU': 0},\n",
    "                        intra_op_parallelism_threads= NUM_WORKER, # num of cores per socket\n",
    "                        inter_op_parallelism_threads=2,\n",
    "                        allow_soft_placement=True)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "# configuration as suggested here:\n",
    "# https://software.intel.com/en-us/articles/tips-to-improve-performance-for-popular-deep-learning-frameworks-on-multi-core-cpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(in_dim):\n",
    "    \"\"\"Define simple model\"\"\"\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(in_dim,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # show info\n",
    "    model.summary()\n",
    "    # compile model\n",
    "    model.compile(loss = mae_wrap_angle,\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = [mse_wrap_angle])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set train parameters and load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "batch_size = 1024\n",
    "num_epochs = 200\n",
    "\n",
    "# define filename of file containing dataset\n",
    "filename = '../../generated/database/raw/raw_nf10_mid/database_raw_nf10_scaledMM.h5'\n",
    "\n",
    "# list of substrings of parameters to select, e.g. 'NFCHOA', 'pos10', 'R006'\n",
    "valid_subset = [] #['LWFS','R006','M006','M027','R027','M013','R013']\n",
    "test_subset = ['NFCHOA_R006', 'NFCHOA_M027']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data indices and reference tables\n",
    "ID_ref, pos_table, cond_table, par = load_raw_IDs_h5(filename)\n",
    "# create test/validation/train dataset split\n",
    "partition = create_split(ID_ref,cond_table,test_subset,valid_split=0.2)\n",
    "# load feature and target data\n",
    "feature_data, target_data, _ = load_raw_ft_h5(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on batches of data set with data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define generators\n",
    "params = {'dim': feature_data.shape[1],\n",
    "          'batch_size': batch_size,\n",
    "          'feature_data': feature_data.values,\n",
    "          'target_data' : target_data.values,\n",
    "          'shuffle': True,\n",
    "          'n_frames': par['nFrames']\n",
    "         }\n",
    "train_batch_generator = DataGenerator_raw(partition['train'], **params)\n",
    "valid_batch_generator = DataGenerator_raw(partition['validation'], **params)\n",
    "\n",
    "# create model\n",
    "model = simple_model(feature_data.shape[1])\n",
    "\n",
    "# define callbacks\n",
    "csv_logger = keras.callbacks.CSVLogger('log.csv')\n",
    "e_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', mode='min', patience=20)\n",
    "cb_list = [csv_logger, e_stop]\n",
    "\n",
    "# start training\n",
    "history = model.fit_generator(generator = train_batch_generator,\n",
    "                              epochs = num_epochs,\n",
    "                              verbose = 1,\n",
    "                              validation_data = valid_batch_generator,\n",
    "                              callbacks = cb_list,\n",
    "                              use_multiprocessing = True,\n",
    "                              workers = NUM_WORKER,\n",
    "                              max_queue_size = 100\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "#### For further evaluation, dedicated notebooks are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss (mae w angle wrapping)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['mae_wrap_angle'])\n",
    "plt.plot(history.history['val_mae_wrap_angle'])\n",
    "plt.title('Model loss metric mse w angle wrapping')\n",
    "plt.ylabel('MSE-W')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "test_batch_generator = DataGenerator_raw(partition['test'], **params)\n",
    "score = model.evaluate_generator(test_batch_generator, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test mse w wrap:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model to file\n",
    "\n",
    "Model naming convention of this project:\n",
    " - m128-128_LOSS_OPTIMIZER_bsBATCHSIZE_t-TESTDATASET_PARAMETER_SCENARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'm128-128_maew_adam_bs1024_t-NR006-NM027'\n",
    "# Save train history to json file\n",
    "import json\n",
    "with open('../data/models_trained/'+model_name+'_history.json', 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "    \n",
    "# Save model\n",
    "from keras.models import load_model\n",
    "model.save('../data/models_trained/'+model_name+'.h5')\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
