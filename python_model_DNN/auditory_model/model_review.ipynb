{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of trained Neural Network based Auditory Models\n",
    "\n",
    "## An Auditory Model for  Azimuthal Localisation in Sound Field Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from utils.custom_loss import mae_wrap_angle, mse_wrap_angle, angle_diff_deg\n",
    "from utils.load_data_raw import DataGenerator_raw, load_raw_all_h5\n",
    "from utils.dataset_split import *\n",
    "from utils.eval import *\n",
    "from utils.utils import get_filelist, open_json\n",
    "\n",
    "NUM_WORKER = 4\n",
    "# define custom_objects to load the custom loss functions with keras\n",
    "custom_obj={'mse_wrap_angle': mse_wrap_angle, 'mae_wrap_angle': mae_wrap_angle}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filelist\n",
    "model_dir = '../data/models_trained/'\n",
    "filelist = get_filelist(model_dir)\n",
    "\n",
    "# load data\n",
    "dset_dir = '../../generated/database/raw/database_raw_scaledMM.h5'\n",
    "feat, targ, ID_ref, pos_t, cond_t, par = load_raw_all_h5(dset_dir)\n",
    "\n",
    "# load unscaled data\n",
    "dset_nsc_dir = '../../generated/database/raw/database_raw.h5'\n",
    "feat_nsc, targ_nsc, _, _, _, _ = load_raw_all_h5(dset_nsc_dir)\n",
    "\n",
    "# create parameter dicts for the test batch generators\n",
    "params = create_test_params(feat, targ, par, shuffle=False)\n",
    "params_nsc = create_test_params(feat_nsc, targ_nsc, par, shuffle=False)\n",
    "\n",
    "cn = feat.columns.tolist()\n",
    "\n",
    "part = {}\n",
    "for i in range(len(cond_t)):\n",
    "    cond = cond_t.iloc[i]\n",
    "    cond_ids_test, pos_ids_test, subject_ids_test = get_subset_ids([cond[0]], cond_t)\n",
    "    part[cond[0]] = get_subset_sample_idx(ID_ref, cond_ids_test, pos_ids_test, subject_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract model and corresponding history and test partition filenames from filelist\n",
    "m_flist = [x for x in filelist if ('history' not in x) and ('partition_test' not in x)]\n",
    "h_flist = ['_' for x in np.zeros(len(m_flist))]\n",
    "pt_flist = ['_' for x in np.zeros(len(m_flist))]\n",
    "for file in filelist:\n",
    "        if ('history' in file):\n",
    "            name = file.replace('_history.json', '.h5')\n",
    "            h_flist[m_flist.index(name)] = file\n",
    "        if ('partition_test' in file):\n",
    "            name = file.replace('_partition_test.json', '.h5')\n",
    "            pt_flist[m_flist.index(name)] = file\n",
    "\n",
    "# seperate model list based on test type (topology/wrapping/normalization)\n",
    "m_flist_tt = [x for x in m_flist if 'toptest' in x]\n",
    "m_flist_nt = [x for x in m_flist if 'normtest' in x]\n",
    "m_flist_wt = [x for x in m_flist if 'wraptest' in x]\n",
    "\n",
    "h_flist_tt = [h_flist[m_flist.index(i)] for i in m_flist_tt]\n",
    "h_flist_nt = [h_flist[m_flist.index(i)] for i in m_flist_nt]\n",
    "h_flist_wt = [h_flist[m_flist.index(i)] for i in m_flist_wt]\n",
    "\n",
    "pt_flist_tt = [pt_flist[m_flist.index(i)] for i in m_flist_tt]\n",
    "pt_flist_nt = [pt_flist[m_flist.index(i)] for i in m_flist_nt]\n",
    "pt_flist_wt = [pt_flist[m_flist.index(i)] for i in m_flist_wt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_filelist(flist):\n",
    "    \"\"\"Print tuple of filelists incl index.\"\"\"\n",
    "    if not isinstance(flist,tuple):\n",
    "        flist = (flist,)\n",
    "        \n",
    "    for i in range(len(flist[0])):\n",
    "        s = ' -' if i<10 else '-'\n",
    "        for j in range(len(flist)):\n",
    "            print(i, s, flist[j][i])\n",
    "\n",
    "# print model list\n",
    "print_filelist((m_flist,h_flist,pt_flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_m_h_pt(model_dir, m_flist, h_flist, pt_flist):\n",
    "    \"\"\"Load models from model list and corresponding history and test partition.\"\"\"\n",
    "    m = []\n",
    "    h = ['_' for x in np.zeros(len(m_flist))]\n",
    "    p = ['_' for x in np.zeros(len(m_flist))]\n",
    "    for i,j in enumerate(m_flist):\n",
    "        m.append(load_model(model_dir + j, custom_objects=custom_obj))\n",
    "        if h_flist[i] != '_':\n",
    "            h[i] = open_json(model_dir, h_flist[i])\n",
    "        if pt_flist[i] != '_':\n",
    "            p[i] = open_json(model_dir, pt_flist[i])\n",
    "        \n",
    "    return m, h, p\n",
    "\n",
    "# load models\n",
    "m_nt, h_nt, pt_nt = load_m_h_pt(model_dir, m_flist_nt, h_flist_nt, pt_flist_nt)\n",
    "m_wt, h_wt, pt_wt = load_m_h_pt(model_dir, m_flist_wt, h_flist_wt, pt_flist_wt)\n",
    "m_tt, h_tt, pt_tt = load_m_h_pt(model_dir, m_flist_tt, h_flist_tt, pt_flist_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: normalized vs. not normalized\n",
    "\n",
    "#### Evaluate the performance of feature scaling to [-1, 1]\n",
    "\n",
    "    Model 1: 128-128-1    m128-128_maew_normtest_neg  vs.  m128-128_maew_normtest_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_filelist(m_flist_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 not normalized: m128-128_maew_normtest_neg\n",
    "tbg128_maew_neg = model_complete_eval(m_nt[0], h_nt[0], pt_nt[0], params_nsc, batch_size=1024,\n",
    "                                      add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_nt[0], tbg128_maew_neg, b_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model 1 normalized: m128-128_maew_normtest_sc\n",
    "tbg128_maew_sc = model_complete_eval(m_nt[1], h_nt[1], pt_nt[1], params, batch_size=1024,\n",
    "                                     add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_nt[1], tbg128_maew_sc, b_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: non-custom vs. custom loss function\n",
    "\n",
    "#### Evaluate the performance of the custom loss function\n",
    "\n",
    "    Model 1: 64-32-1      m064-032_mae_wraptest  vs.  m064-032_maew_wraptest\n",
    "    Model 2: 128-128-1    m128-128_mae_wraptest  vs.  m128-128_maew_wraptest\n",
    "    Model 3: 256-128-1    m256-128_mae_wraptest  vs.  m256-128_maew_wraptest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_filelist(m_flist_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model 1 mae loss: m064-032_mae\n",
    "tbg64_mae = model_complete_eval(m_wt[0], h_wt[0], pt_wt[0], params, batch_size=1024,\n",
    "                                 add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_wt[0], tbg64_maew, b_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 custom loss: m64-032_maew\n",
    "tbg64_maew = model_complete_eval(m_wt[1], h_wt[1], pt_wt[1], params, batch_size=1024,\n",
    "                                 add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_wt[1], tbg64_mae, b_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 mae loss: m128-128_mae\n",
    "tbg128_mae = model_complete_eval(m_wt[2], h_wt[2], pt_wt[2], params, batch_size=1024,\n",
    "                                 add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_wt[2], tbg128_mae, b_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 custom loss: m128-128_maew\n",
    "tbg128_maew = model_complete_eval(m_wt[3], h_wt[3], pt_wt[3], params, batch_size=1024,\n",
    "                                  add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_wt[3], tbg128_maew, b_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 mae loss: m256-128_mae\n",
    "tbg256_mae = model_complete_eval(m_wt[4], h_wt[4], pt_wt[4], params, batch_size=1024,\n",
    "                                 add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_wt[4], tbg256_mae, b_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 custom loss: m256-128_maew\n",
    "tbg256_maew = model_complete_eval(m_wt[5], h_wt[5], pt_wt[5], params, batch_size=1024,\n",
    "                                  add_metrics= [mae_wrap_angle, mse_wrap_angle], workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_wt[5], tbg256_maew, b_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Model/Net Topology\n",
    "\n",
    "#### Evaluate different model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_filelist(m_flist_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose model by id and evaluate over all positions\n",
    "i = 12\n",
    "tbg_tt = model_complete_eval(m_tt[i], h_tt[i], part['NFCHOA_M027'], params, batch_size=1000, workers=NUM_WORKER)\n",
    "\n",
    "#predict on model\n",
    "#pred, y = model_pred_on_gen_batch(m_tt[i], tbg_tt, b_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model by id and evaluate for each position seperately\n",
    "i = 6\n",
    "mae_p, mse_p, loc_p = model_eval_pos(m_tt[i], h_tt[i], part['NFCHOA_R006'], params, ID_ref, batch_size=1000, workers=NUM_WORKER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error evaluation for different models across architechtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get IDs of subset of data per position of one condition\n",
    "part_x_M027, pos_ids = get_pos_IDs(part['NFCHOA_M027'],ID_ref)\n",
    "part_x_R006, _ = get_pos_IDs(part['NFCHOA_R006'],ID_ref)\n",
    "\n",
    "# set parameters\n",
    "params_t = params.copy()\n",
    "params_t['batch_size'] = 1000\n",
    "params_t_nsc = params_nsc.copy() # unscaled\n",
    "params_t_nsc['batch_size'] = 1000\n",
    "params_t_nic = create_test_params(feat[cn[:64]], targ, par, batch_size=1000, shuffle=False) # training w/o IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate MSE = squared bias + variance\n",
    "n_model = len(m_flist_tt)\n",
    "MSE_R006 = np.zeros(n_model)\n",
    "tau_sq_R006 = np.zeros(n_model)\n",
    "sigma_sq_R006 = np.zeros(n_model)\n",
    "MSE_M027 = np.zeros(n_model)\n",
    "tau_sq_M027 = np.zeros(n_model)\n",
    "sigma_sq_M027 = np.zeros(n_model)\n",
    "\n",
    "for i in np.arange(n_model):\n",
    "    # special condition handling\n",
    "    if 'no-ic' in m_flist_tt[i]:\n",
    "        params_tmp = params_t_nic\n",
    "    elif 'nsc' in m_flist_tt[i]:\n",
    "        params_tmp = params_t_nsc\n",
    "    else:\n",
    "        params_tmp = params_t\n",
    "    # calculation\n",
    "    model = load_model(model_dir + m_flist_tt[i], custom_objects={'mse_wrap_angle': mse_wrap_angle, 'mae_wrap_angle': mae_wrap_angle})\n",
    "    MSE_R006[i], tau_sq_R006[i], sigma_sq_R006[i], _, _ = calc_errors_m(model, part_x_R006, params_tmp, pos_ids, verbose=0, workers=NUM_WORKER)\n",
    "    MSE_M027[i], tau_sq_M027[i], sigma_sq_M027[i], _, _ = calc_errors_m(model, part_x_M027, params_tmp, pos_ids, verbose=0, workers=NUM_WORKER)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_nlist = []\n",
    "loss_f = []\n",
    "model_str = []\n",
    "test_dset = []\n",
    "bs = []\n",
    "for i in np.arange(n_model):\n",
    "    strn = m_flist[i].split('_')\n",
    "    loss_f.append('{:<5}'.format(strn[1]))\n",
    "    model_str.append('{:<15}'.format(strn[0]))\n",
    "    test_dset.append('{:<15}'.format(strn[4]))\n",
    "    bs.append('{:<5}'.format(strn[3]))\n",
    "    m_nlist.append(strn[0]+'_'+strn[1]+'_'+strn[4])\n",
    "\n",
    "data = {\n",
    "    'model':model_str,\n",
    "    'loss_f':loss_f,\n",
    "    'test_dset':test_dset,\n",
    "    'bs':bs,\n",
    "    'MSE_R006' : MSE_R006,\n",
    "    'sq_bias_R006': tau_sq_R006,\n",
    "    'variance_R006': sigma_sq_R006,\n",
    "    'MSE_M027': MSE_M027,\n",
    "    'sq_bias_M027': tau_sq_M027,\n",
    "    'variance_M027': sigma_sq_M027    \n",
    "}\n",
    "mse_df = pd.DataFrame(data)\n",
    "mse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RMSE\n",
    "rmse_df = mse_df.copy()\n",
    "c = ['MSE_R006', 'sq_bias_R006', 'variance_R006', 'MSE_M027', 'sq_bias_M027', 'variance_M027']\n",
    "cn = ['RMSE_R006', 'bias_R006', 'std_R006', 'RMSE_M027', 'bias_M027', 'std_M027']\n",
    "for i,cl in enumerate(c):\n",
    "    rmse_df[cl] = np.sqrt(rmse_df[cl])\n",
    "    rmse_df.rename(columns={cl: cn[i]}, inplace=True)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
